{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Experiments with Temperature in LLMs\n",
    "topic: LLM\n",
    "author: Subhaditya Mukherjee\n",
    "date: 07-08-2024\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Introduction\n",
    "Over the past few months at OpenML, we have been experimenting with LLM models in an attempt to improve the search experience for our users. While our existing implementation uses ElasticSearch, we wanted to also have the option of having a more \"semantic\" search experience. \n",
    "\n",
    "Aside from the usual RAG pipeline that everyone and their grandparents seems to be using these days, we also wanted to experiment with using an LLM to semi-automatically generate filters for our search queries. While it may not seem like a big feature, it is something that has always been a bit of an annoyance for some of our users. \n",
    "\n",
    "So what does this entail? Consider the interface we have at the moment. We have a search bar at the top, and subsequently a bunch of filters that users can use to narrow down their search. While this works pretty well as is, how about trying to automate it a bit.\n",
    "\n",
    "In summary, we want a query like \"find me a large dataset with multiple classes of flowers\" to automatically generate filters like \"classification\", \"multiclass\", \"sort by size of dataset\" etc.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img alt=\"\" src=\"../images/blogs/temperature/1.png\" style=\"width:40%\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Temperature\n",
    "\n",
    "Think about the first time you used ChatGPT. What stood out to you? Was it how well it could elaborate on a topic? Or was it how creative it could be? The temperature parameter in LLMs is what controls this. \n",
    "\n",
    "How can we control creativity? Well, saying that we can directly control creativity is a bit of a stretch. We can however use a workaround.\n",
    "\n",
    "Do you remember the softmax function? The function that takes a vector of arbitrary real-valued scores and squashes it into a vector of probabilities that sum to 1. The inputs to the softmax function are the unnormalized log likelikhoods or the raw per class score assigned by the model. \n",
    "\n",
    "The softmax function is defined as:\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{k} e^{x_j}}$$\n",
    "\n",
    "If we want more control over the distribution of the probabilities, we can use a temperature parameter. This would look like:\n",
    "\n",
    "$$ \\text{softmax}(x_i) = \\frac{e^{x_i/T}}{\\sum_{j=1}^{k} e^{x_j/T}} $$\n",
    "\n",
    "where $T$ is the temperature parameter.\n",
    "\n",
    "- If $T = 1$, the softmax function is the same as the original softmax function.\n",
    "\n",
    "- If $T > 1$, the probabilities will become \"flatter\". Since the difference between the probabilities will be less, the model can be more exploratory aka more creative.\n",
    "\n",
    "- If $T < 1$, the distribution of the probabilities are \"peakier\". There will be a higher difference between the probabilities, leading to the model being more confident in its predictions, but also less creative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Temperature Using Softmax\n",
    "\n",
    "```python\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "# LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "sns.set_theme(style=\"white\")\n",
    "```\n",
    "\n",
    "```python\n",
    "def softmax(input, t=1.0):\n",
    "  ex = np.exp(input/t)\n",
    "  sum = np.sum(ex, axis=0)\n",
    "  return ex / sum\n",
    "```\n",
    "\n",
    "```python\n",
    "# plot softmax over a range of inputs\n",
    "x = np.arange(0,1.0, 0.01)\n",
    "t = np.array([0.1,.5, .8, 1.0])\n",
    "y = np.array([softmax(x, ti) for ti in t])\n",
    "\n",
    "# Create a DataFrame for Seaborn\n",
    "data = pd.DataFrame({\n",
    "    'x': np.tile(x, len(t)),\n",
    "    'softmax': np.concatenate(y),\n",
    "    't': np.repeat(t, len(x))\n",
    "})\n",
    "\n",
    "# Plotting with Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(data=data, x='x', y='softmax', hue='t', palette='viridis')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('softmax(x)')\n",
    "plt.toc: true\n",
    "title('Softmax Function for Different Values of t')\n",
    "plt.legend(toc: true\n",
    "title='t')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img alt=\"\" src=\"../images/blogs/temperature/2.png\" style=\"width:40%\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Creating the Experimental Setup\n",
    "Now, we can focus on testing the effects of temperature for our use case. We are using the `llama3` model for our experiments. The experiments are being run on a 2023 MacBook Pro with an M3 chip and 18GB memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Prompt\n",
    "We need to first think of a prompt that we can use for our experiments. This prompt can be thought of as an instruction that the model uses along with the query to generate answers. To make it easier for us to use, we only want one/two word answers and for now we are only focusing on a small subset of the filters that we want our model to understand.\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"User Query : {query}\n",
    "Based on the query, answer the following questions one by one in one or two words only and a maximum of two with commas only if asked for. Use only the information given and do not make up answers - \n",
    "Does the user care about the size of the dataset? Yes/No and if yes, ascending/descending.\n",
    "Does the user want to sort by number of downloads? Yes/No.\n",
    "Does the user care about missing values? Yes/No.\n",
    "If it seems like the user wants a classification dataset, is it binary/multi-class/multi-label? If not, say none.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "```python\n",
    "query = \"Find me a big classification dataset about mushrooms\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Chain\n",
    "Since we are using the `langchain` and `ollama` libraries for our experiments, we follow their API and create a chain. The template uses string formatting to insert the prompt and the query into the chain.\n",
    "\n",
    "```python\n",
    "def create_chain(prompt , temperature, llm_model = \"llama3\"):\n",
    "    prompt = ChatPromptTemplate.from_template(prompt)\n",
    "    llm = ChatOllama(model=llm_model, temperature=temperature)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    return chain\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Results\n",
    "To make it easier for us to analyze the results, we generate an example answer and then see see if any further processing is needed.\n",
    "\n",
    "```python\n",
    "# functiont to parse responses like this to a list of yes/no/none/yes,aescending/no etc\n",
    "def parse_response(response):\n",
    "    # split by new line and remove first two lines (here are the answers:)\n",
    "    response = response.split('\\n')[2::]\n",
    "    # if response has a question mark, split by question mark and remove empty strings\n",
    "    for i in range(len(response)):\n",
    "        if '?' in response[i]:\n",
    "            response[i] = response[i].split('?')[1].strip()\n",
    "    # replace full stops with empty strings\n",
    "    response = [x.replace('.','') for x in response]\n",
    "    response = [x for x in response if x]\n",
    "    return response\n",
    "```\n",
    "\n",
    "```python\n",
    "chain = create_chain(prompt, 0.5)\n",
    "response = chain.invoke({\"query\": query})\n",
    "print(response)\n",
    "```\n",
    "\n",
    "    Here are the answers:\n",
    "    \n",
    "    1. Does the user care about the size of the dataset?\n",
    "    Yes, ascending.\n",
    "    \n",
    "    2. Does the user want to sort by number of downloads?\n",
    "    No\n",
    "    \n",
    "    3. Does the user care about missing values?\n",
    "    No\n",
    "    \n",
    "    4. Is it a classification dataset? If so, is it binary/multi-class/multi-label?\n",
    "    Yes, multi-class\n",
    "\n",
    "Yay, it works. We now write a function to generate results for different temperatures.\n",
    "\n",
    "```python\n",
    "def generate_results_for_temp(query:str, range_of_temps : np.ndarray) -> List[List[str]:\n",
    "    results = []\n",
    "    for temperature in tqdm(range_of_temps):\n",
    "        chain = create_chain(prompt, temperature)\n",
    "        response = chain.invoke({\"query\": query})\n",
    "        results.append(parse_response(response))\n",
    "    return results\n",
    "        \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Experiments and Plotting Results\n",
    "It is time to run the experiments and plot the results. \n",
    "We write a function to plot the results in a `stripplot` to see the distribution of the answers for different temperatures.\n",
    "\n",
    "```python\n",
    "def plot_yes_no(df: pd.DataFrame, toc: true\n",
    "title:str) -> None:\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    fig.suptoc: true\n",
    "title(toc: true\n",
    "title)\n",
    "    sns.stripplot(data=df, x='size', y='temperature', ax=axs[0, 0], hue='size')\n",
    "    sns.stripplot(data=df, x='sort_by_downloads', y='temperature', ax=axs[0, 1], hue='sort_by_downloads')\n",
    "    sns.stripplot(data=df, x='missing_values', y='temperature', ax=axs[1, 0], hue='missing_values')\n",
    "    sns.stripplot(data=df, x='classification_type', y='temperature', ax=axs[1, 1], hue='classification_type')\n",
    "    # tilt x axis labels\n",
    "    for ax in axs.flat:\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "Sometimes, the model returns an extra field, we combine the last two fields to plot the results. (This is a bit of a hack, but it works for now and is ONLY used for plotting)\n",
    "\n",
    "```python\n",
    "def combine_last_two_elements(lst):\n",
    "    # Check if the list has at least two elements\n",
    "    if len(lst) > 4:\n",
    "        # Combine the last two elements with a space separator\n",
    "        combined_element = lst[-2] + ' ' + lst[-1]\n",
    "\n",
    "        # Create a new list with combined element instead of the last two\n",
    "        return lst[:-2] + [combined_element]\n",
    "    else:\n",
    "        return lst\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "Out first experiment is a rather simple query, \"Find me a big classification dataset about mushrooms\". As you can probably guess, we are looking for a dataset that is large, is a classification dataset and is about mushrooms.\n",
    "\n",
    "```python\n",
    "range_of_temps = np.linspace(0, 1, 20)\n",
    "query = \"Find me a big classification dataset about mushrooms\"\n",
    "results1 = generate_results_for_temp(query, range_of_temps)\n",
    "```\n",
    "\n",
    "    100%|██████████| 20/20 [00:49<00:00,  2.49s/it]\n",
    "\n",
    "```python\n",
    "results1 = [y for y in x if all(sub not in y for sub in [\"If\", \":\"])] for x in results1]\n",
    "```\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame(results1, columns = ['size', 'sort_by_downloads', 'missing_values', 'classification_type'])\n",
    "df['temperature'] = range_of_temps\n",
    "plot_yes_no(df, toc: true\n",
    "title = query)\n",
    "```\n",
    "    \n",
    "<div style=\"text-align:center\">\n",
    "<img alt=\"\" src=\"../images/blogs/temperature/3.png\" style=\"width:40%\">\n",
    "</div>\n",
    "\n",
    "Rather interesting don't you think? At higher temperatures, the model gets the answers wrong. Even at a temperature slightly above 0.1, the model starts adding extra information to it's answers.\n",
    "\n",
    "Did you notice that I tried to remove sentences that started with \"If\"? There are more examples of this later, but this is because at higher temperatures, the model tends to add random sentences to the answers and this makes it quite hard to plot them.\n",
    "\n",
    "### Experiment 2\n",
    "Our second experiment is super easy. \"Find me a dataset that has a lot of missing values and order by number of downloads\". As you can obviously guess, we are looking for a dataset that has a lot of missing values and we want to order the results by the number of downloads.\n",
    "\n",
    "```python\n",
    "range_of_temps = np.linspace(0, 1, 20)\n",
    "query = \"Find me a dataset that has a lot of missing values and order by number of downloads\"\n",
    "results2 = generate_results_for_temp(query, range_of_temps)\n",
    "results2 = [y for y in x if \"so\" not in y] for x in results2]\n",
    "\n",
    "```\n",
    "\n",
    "    100%|██████████| 20/20 [00:34<00:00,  1.74s/it]\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame(results2, columns = ['size', 'sort_by_downloads', 'missing_values', 'classification_type'])\n",
    "df['temperature'] = range_of_temps\n",
    "plot_yes_no(df, toc: true\n",
    "title = query)\n",
    "```\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img alt=\"\" src=\"../images/blogs/temperature/4.png\" style=\"width:40%\">\n",
    "</div>\n",
    "\n",
    "Hmm, same as before. The model starts adding extra information at higher temperatures and starts getting the answers wrong. (Yes, No?? ) What kind of answer is that?\n",
    "\n",
    "## Experiment 3\n",
    "- Now a slightly more complex query. \"Find me a dataset that has 10 classes and sort by number of downloads\". We want it to understand that we want a multiclass classification dataset and we want to sort the results by the number of downloads.\n",
    "\n",
    "```python\n",
    "range_of_temps = np.linspace(0, 1, 20)\n",
    "query = \"Find me a dataset that has 10 classes and sort by number of downloads\"\n",
    "results3 = generate_results_for_temp(query, range_of_temps)\n",
    "```\n",
    "\n",
    "    100%|██████████| 20/20 [00:55<00:00,  2.80s/it]\n",
    "\n",
    "```python\n",
    "results3 = [combine_last_two_elements(x) for x in results3]\n",
    "```\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame(results3, columns = ['size', 'sort_by_downloads', 'missing_values', 'classification_type'])\n",
    "df['temperature'] = range_of_temps\n",
    "plot_yes_no(df, toc: true\n",
    "title = query)\n",
    "```\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img alt=\"\" src=\"../images/blogs/temperature/5.png\" style=\"width:40%\">\n",
    "</div>\n",
    "\n",
    "This seems to have been very easy for the model. But as always, the model starts adding extra information at higher temperatures. A lot of extra information in fact. Even though the prompt says to ONLY answer with one or two words\n",
    "\n",
    "## Experiment 4\n",
    "- \"Find me a dataset that 2 classes and is a big dataset\". You know the drill by now. We want a binary classification dataset that is large.\n",
    "\n",
    "```python\n",
    "range_of_temps = np.linspace(0, 1, 20)\n",
    "query = \"Find me a dataset that 2 classes and is a big dataset\"\n",
    "results4 = generate_results_for_temp(query, range_of_temps)\n",
    "```\n",
    "\n",
    "    100%|██████████| 20/20 [00:42<00:00,  2.14s/it]\n",
    "\n",
    "```python\n",
    "results4 = [combine_last_two_elements(x) for x in results4]\n",
    "```\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame(results4, columns = ['size', 'sort_by_downloads', 'missing_values', 'classification_type'])\n",
    "df['temperature'] = range_of_temps\n",
    "plot_yes_no(df, toc: true\n",
    "title = query)\n",
    "```\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img alt=\"\" src=\"../images/blogs/temperature/6.png\" style=\"width:40%\">\n",
    "</div>\n",
    "\n",
    "Notice how some things changed? At higher temperatures, we get extended answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, we can see that we should probably stick to lower temperatures for our use case. As we go higher, the model starts being more \"creative\" and either adds extra information to the answers or gets them wrong. While this behaviour might be useful in cases like creative writing, it is not something we want in our search.\n",
    "\n",
    "Using LLMs can sometimes be a bit of a hit or miss. But of course, learning to control it's parameters can help us get the most out of it. This blog post was just a simple experiment, but in the deluge of content made by people who have no idea what Softmax is, I hope this was helpful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
